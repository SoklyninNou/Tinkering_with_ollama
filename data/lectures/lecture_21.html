<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>
  <body>
    <h2>Linear Classifier</h2>
    <p>A linear classifier makes its predictions based on a linear combination of the input features:</p>
    <p>Where:</p>
    <ul>
      <li>
        <p>is the bias term.</p>
      </li>
      <li>
        <p>are the weights associated with each feature.</p>
      </li>
      <li>
        <p>are the input features.</p>
      </li>
      <li>
        <p>is the output score.</p>
      </li>
    </ul>
    <h3>Decision Rule</h3>
    <p>The decision rule for a linear classifier is typically based on the sign of the output score</p>
    <p>:</p>
    <ul>
      <li>
        <p>If</p>
        <p>, predict class 1.</p>
      </li>
      <li>
        <p>If</p>
        <p>, predict class 0.</p>
      </li>
    </ul>
    <p>We can visualize the decision boundary as a hyperplane in the feature space.</p>
    <h3>Weight Updates / Perceptrons</h3>
    <p>The perceptron algorithm is an online learning algorithm that updates the weights based on the prediction error for each training example. The update rule is designed to adjust the weights in a way that reduces future misclassifications.</p>
    <p>Properties of the perceptron learning rule:</p>
    <ul>
      <li><strong>Separability:</strong> The perceptron is separable if some parameters correctly classify all training data.</li>
      <li><strong>Convergence:</strong> The perceptron algorithm converges to a solution if the data is linearly separable.</li>
      <li><strong>Mistake Bound:</strong> The maximum number of mistakes relates to the margin or degree of separability.</li>
    </ul>
    <p>Some problems with the perceptron algorithm:</p>
    <ul>
      <li><strong>Noise:</strong> if the data is not separable, the perceptron may oscillate and never converge.</li>
      <li><strong>Margin:</strong> perceptrons doesnâ€™t maximize the margin between classes, leading to poor generalization.</li>
    </ul>
    <h4>Binary Perceptrons</h4>
    <p>The binary perceptron algorithm updates the weights based on the prediction error:</p>
    <ul>
      <li>
        <p>For each training example</p>
        <p>:</p>
        <ul>
          <li>
            <p>Classify with current weights to get prediction</p>
            <p>.</p>
          </li>
          <li>
            <p>If</p>
            <p>(misclassification):</p>
            <ul>
              <li>
                <p>Update weights:</p>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
    <h4>Multiclass Perceptrons</h4>
    <p>For multiclass classification, the perceptron algorithm updates weights for each class:</p>
    <ul>
      <li>
        <p>For each training example</p>
        <p>:</p>
        <ul>
          <li>
            <p>Classify with current weights to get predicted class</p>
            <p>.</p>
          </li>
          <li>
            <p>If</p>
            <p>(misclassification):</p>
            <ul>
              <li>
                <p>Update weights for true class:</p>
              </li>
              <li>
                <p>Update weights for predicted class:</p>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
    <h3>Logistic Regression</h3>
    <p>When the data is not linearly separable, the perceptron algorithm may fail to converge. To address this, we can use a Probabilistic Decision model such as Logistic Regression:</p>
    <p>When interpreting scores to probabilities, we use the logistic (sigmoid) function:</p>
  </body>
</html>
