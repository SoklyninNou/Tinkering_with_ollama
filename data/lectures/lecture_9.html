<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>
  <body>
    <h3>Bellman Equations</h3>
    <p>Definition of “optimal utility” via Expectimax recurrence gives a simple one-step look ahead relationship amongst optimal utility values:</p>
    <h3>Policy Method/Evaluation</h3>
    <p>One variance of policy evaluation is for a fixed policy:</p>
    <p>notice how we are no longer maximizing over the possible actions since we have a fixed policy. This reduces the time complexity to:</p>
    <p>. This is also a linear system of equation.</p>
    <h3>Policy Extraction</h3>
    <p>What we can also do is to find what policy is optimal at each state:</p>
    <p>This is called <strong>Policy Extraction</strong>. Given the q-values, we can also compute it by:</p>
    <p>this is more trivial since</p>
    <p>tells you the utility of each action.</p>
    <h3>Policy Iteration</h3>
    <p><strong>Policy iteration</strong> is a method of finding the optimal policy. The problem with value iteration is that it is slow to converge values, bottle necking the policies. In Policy iteration, we alternate between two steps:</p>
    <ol>
      <li value="1"><strong>Policy evaluation:</strong> calculate utilities for some fixed policy (not optimal utilities!) until convergence</li>
    </ol>
    <ol>
      <li value="2"><strong>Policy improvement:</strong> update policy using one-step look-ahead with resulting converged (but not optimal!) utilities as future values</li>
    </ol>
    <p>This method converges much faster under some conditions.</p>
    <p>In Policy Iteration, we run with a fixed policy and compute the utility of each state given that policy. We then find the best expected action for each room and change the policy accordingly. Repeat until the best expected action for each room is the policy we just ran.</p>
  </body>
</html>
