<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>
  <body>
    <h2>Reinforcement Learning</h2>
    <p>In <strong>Reinforcement Learning</strong> we assume a MDP:</p>
    <ul>
      <li>
        <p>A set of states</p>
      </li>
      <li>
        <p>A set of actions (per state)</p>
      </li>
      <li>
        <p>A model</p>
      </li>
      <li>
        <p>A reward function</p>
      </li>
    </ul>
    <p>The difference is that we don’t know the transition or reward function. Reinforcement learning takes into account the following concepts:</p>
    <ul>
      <li><strong>Exploration:</strong> perform unknown actions to gather information</li>
      <li><strong>Exploitation:</strong> Based on current information, perform the optimal action</li>
      <li><strong>Regret:</strong> the loss between the best action and performed action</li>
      <li><strong>Sampling:</strong> Perform action repeatedly to estimate it better</li>
      <li><strong>Difficulty:</strong> learning can be much harder than solving a known MDP</li>
    </ul>
    <h3>Model-Based Learning</h3>
    <p>The idea behind <strong>Model-based Learning</strong> is that we make an approximate model based on what we know about the reward and transition functions. We then run the model as is.</p>
    <ol>
      <li value="1">
        <p><strong>Step 1:</strong> Take actions</p>
        <p>at state</p>
        <p>and estimate</p>
        <p>, get</p>
        <p>when we reach</p>
      </li>
      <li value="2"><strong>Step 2:</strong> Solve the MDP using the estimated reward and transition functions</li>
    </ol>
    <h3>Model Free Learning/Passive Reinforcement Learning</h3>
    <p>In <strong>Passive Reinforcement Learning</strong>, we are given a fixed policy to learn the reward and transition functions. We could do this by <strong>Direct Evaluation</strong>, where we average reward of running that policy at each initial state.</p>
    <p>This is a simple algorithm that is easy to run but faces some problems. Some problems are that we might get unlucky when evaluating the value of a state and each state is learned separately.<br><br><br></p>
    <h4>Sample-Based Policy Evaluation</h4>
    <p>Since we don’t know the transition function, we can take a sample of our past actions onto successor states to get an estimate of the function:</p>
    <h3>Temporal Difference Learning / Exponential Moving Average</h3>
    <p>The previous strategy we did was to run a certain amount of episodes, then extract information from the runs. <strong>Temporal Difference Learning</strong> updates</p>
    <p>each time we experience transition</p>
    <p>:</p>
    <p>What this means is that our estimate for state</p>
    <p>is a weighted sum of our sample of</p>
    <p>we just observed and the old estimate we had before, where the value of alpha is importance of the new value.</p>
    <p>In <strong>Exponential Moving Average</strong>, we can compute the average of the states we see by running interpolation update:</p>
    <p>This puts more emphasis on the most recent sample, with more distant sample decaying exponentially.</p>
    <p>tends to be a small number and gets smaller over time so that the averages converge.</p>
    <p>Temporal Difference Learning gives us a good way of estimating values of individual states, but it doesn’t allow for policy extraction like other methods. To fix this, we should estimate the <strong>Q-value</strong>, which describes the values of an action and state pair, instead of just the value.</p>
    <h3>Active Reinforcement Learning</h3>
    <p>Unlike passive reinforcement learning, where a fixed policy is given to us, <strong>Active Reinforcement Learning</strong> makes us decide the next action by ourselves. Just like passive reinforcement learning. We are not given the reward nor the transition function</p>
    <h4>Q-Value Iteration</h4>
    <p>In <strong>Q-Value Iteration</strong>, we start we</p>
    <p>as the base case and calculate:</p>
    <p>We learn</p>
    <p>values as we go:</p>
    <ul>
      <li>receive a sample (s, a, s’, r)</li>
      <li>Consider your old estimate: Q(s, a)</li>
      <li>
        <p>Consider your new sample estimate:</p>
      </li>
    </ul>
    <p>Q-learning converges to optimal policy after enough iterations. This is called <strong>Off-Policy learning</strong>.</p>
  </body>
</html>
