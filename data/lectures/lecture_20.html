<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>
  <body>
    <h2>Classification</h2>
    <p>In machine learning, classification is a supervised learning task where the goal is to predict the categorical label of an input based on its features.</p>
    <h3>Naive Bayes Classifier</h3>
    <p>The Naive Bayes classifier is a simple yet effective probabilistic classifier based on Bayes’ theorem with the “naive” assumption of feature independence.</p>
    <p>The model computes the posterior probability of each class given the input features and assigns the class with the highest probability. The formula for the Naive Bayes classifier is given by:</p>
    <p>Where:</p>
    <ul>
      <li>
        <p>is the class label.</p>
      </li>
      <li>
        <p>are the features.</p>
      </li>
      <li>
        <p>is the prior probability of class</p>
        <p>.</p>
      </li>
    </ul>
    <h3>Inference for Naive Bayes</h3>
    <p>To classify a new instance with features</p>
    <p>, we compute the posterior probability for each class</p>
    <p>:</p>
    <h3>General Naive Bayes</h3>
    <p>What we neeed to use Naive Bayes is:</p>
    <ol>
      <li value="1">
        <p>Inference method:</p>
        <ul>
          <li>
            <p>Compute</p>
            <p>for each class</p>
            <p>.</p>
          </li>
          <li>
            <p>Compute</p>
            <p>for each feature</p>
            <p>and class</p>
            <p>.</p>
          </li>
        </ul>
      </li>
      <li value="2">
        <p>Estimates of local conditional probabilisty tables:</p>
        <ul>
          <li>
            <p>: Estimated from the training data as the frequency of each class.</p>
          </li>
          <li>
            <p>: Estimated from the training data as the frequency of each feature value given each class.</p>
          </li>
        </ul>
      </li>
    </ol>
    <p>Naive Bayes assumes that all features are conditionally independent given the class label.<br>One detail is that we take the logarithm of the probabilities to avoid numerical underflow and convert products into sums:</p>
    <p>Where</p>
    <p>is a constant that does not depend on</p>
    <p>.</p>
    <h3>Traing and Testing</h3>
    <p>Empirical Risk Minimization (ERM) is a principle in statistical learning theory that aims to find a hypothesis that minimizes the empirical risk, which is the average loss over the training data. But we might overfit the training data if we only focus on minimizing the empirical risk.</p>
    <p>To evaluate the performance of a classifier, we typically split the dataset into training, held out, and testing sets. The training set is used to train the model, the held out set is used for model selection and hyperparameter tuning, and the testing set is used to evaluate the final performance of the model.</p>
    <p>Overfitting occurs when a model learns the training data too well, including its noise and outliers, leading to poor generalization to new, unseen data.</p>
    <h3>Parameter Estimation</h3>
    <p>Parameter estimation for the Naive Bayes classifier involves estimating the probabilities</p>
    <p>and</p>
    <p>from the training data. There are two common methods for parameter estimation: Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP) estimation.</p>
    <h4>Maximum Likelihood Estimation (MLE)</h4>
    <p>Maximum Likelihood Estimation (MLE) is a method for estimating the parameters of a statistical model by maximizing the likelihood function, which measures how well the model explains the observed data.</p>
    <h4>Maximum A Posteriori (MAP) Estimation</h4>
    <p>Maximum A Posteriori (MAP) estimation is a method for estimating the parameters of a statistical model by maximizing the posterior distribution, which combines the likelihood of the observed data with a prior distribution over the parameters.</p>
    <h3>Laplace Smoothing</h3>
    <p>One common issue in parameter estimation is the zero-frequency problem, where a feature value does not appear in the training data for a particular class, leading to a probability of zero. To address this, we can use smoothing techniques such as Laplace smoothing, which adds a small constant to all counts to ensure that no probability is zero. This makes each estimates less extreme, which makes it more uniform.</p>
    <h3>Confidence</h3>
    <p>In addition to predicting the class label, the Naive Bayes classifier can also provide a measure of confidence in its predictions by computing the posterior probabilities for each class. The confidence can be interpreted as the probability that the predicted class is correct given the input features.</p>
  </body>
</html>
