Bellman Equations
Definition of “optimal utility” via Expectimax recurrence gives a simple one-step look ahead relationship amongst optimal utility values:
Policy Method/Evaluation
One variance of policy evaluation is for a fixed policy:
notice how we are no longer maximizing over the possible actions since we have a fixed policy. This reduces the time complexity to:
. This is also a linear system of equation.
Policy Extraction
What we can also do is to find what policy is optimal at each state:
This is called
Policy Extraction
. Given the q-values, we can also compute it by:
this is more trivial since
tells you the utility of each action.
Policy Iteration
Policy iteration
is a method of finding the optimal policy. The problem with value iteration is that it is slow to converge values, bottle necking the policies. In Policy iteration, we alternate between two steps:
Policy evaluation:
calculate utilities for some fixed policy (not optimal utilities!) until convergence
Policy improvement:
update policy using one-step look-ahead with resulting converged (but not optimal!) utilities as future values
This method converges much faster under some conditions.
In Policy Iteration, we run with a fixed policy and compute the utility of each state given that policy. We then find the best expected action for each room and change the policy accordingly. Repeat until the best expected action for each room is the policy we just ran.